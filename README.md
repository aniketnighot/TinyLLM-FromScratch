# TinyLLM-FromScratch ğŸš€
*A character-level Transformer Decoder LLM built entirely from scratch using PyTorch.*

---

## ğŸ“š Project Overview

**TinyLLM-FromScratch** is a minimal yet powerful **Transformer Decoder model** that generates text **character-by-character** â€” without using any pre-built architectures.  
This project demonstrates a complete understanding of **LLMs**, **Self-Attention**, and **Transformer architecture**, all implemented **from first principles**.

Trained on song lyrics, the model learns to predict the next character based on previous characters, showcasing the power of attention mechanisms even at a small scale.

---

## ğŸ† Highlights
- âœï¸ Full **Transformer Decoder** implemented manually in PyTorch
- ğŸ§  Custom-built **Self-Attention**, **Multi-Head Attention**, **Feed Forward Networks**
- ğŸ”¥ **Character-level** language modeling without any external tokenizers
- ğŸ› ï¸ **Training loop**, **batching**, and **generation** fully coded from scratch
- ğŸ“– Trained on real Hindi lyrics dataset for creative text generation
- ğŸ“¦ Fully modular and easy-to-extend project structure

---

## ğŸ”¥ Tech Stack
- Python 3.10+
- PyTorch
- Numpy

---

## ğŸ› ï¸ How It Works
1. Raw text data is read and unique characters are tokenized.
2. Inputs are fed into an **embedding layer** + **positional encoding**.
3. Data passes through multiple layers of:
   - **Masked Self-Attention**
   - **Multi-Head Attention**
   - **Feed Forward Networks**
4. Final outputs predict the **next character** probabilities.
5. Sampling from output allows **new text generation**.

---

## ğŸ—‚ï¸ Folder Structure
```
TinyLLM-FromScratch/
â”œâ”€â”€ biogram_model/
â”‚   â”œâ”€â”€ model.py        # Transformer Model
â”‚   â”œâ”€â”€ utils.py        # Tokenizer and helper functions
â”œâ”€â”€ hindi_song_lyrics.txt  # Training dataset
â”œâ”€â”€ train.py            # Model training script
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ GPT_From_Scratch.ipynb  # Full Colab Notebook
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
```

 
---

## Quickstart
1. **Clone this repository**
   
   ```bash
   git clone https://github.com/aniketnighot/TinyLLM-FromScratch.git
   cd TinyLLM-FromScratch

3. **Install Dependencies**
   
   ```bash
   pip install -r requirements.txt

4. **Train the Model**
   
   ```bash
   python train.py

5. **Generate New Text After training, the model will generate ~500 characters of new Hindi lyrics based on learned patterns.**

## ğŸ“ˆ Sample Output

tera naam lene ki aadat ban gayi hai,
meri raaton ki chahat ban gayi hai,
tera intezaar rehne laga hai har pal,
meri khamoshi ki baat ban gayi hai...

## ğŸ§  Future Improvements


1. Train on larger multilingual datasets
2. Add support for word-level tokenization
3. Scale model size (more heads, more layers)




## Acknowledgments
Inspired by Andrej Karpathy's teachings and philosophy.

Self-implemented for deep understanding of LLMs and Transformer models.


